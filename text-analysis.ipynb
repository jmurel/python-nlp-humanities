{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iD3jvI5CfZR"
   },
   "source": [
    "# Text analysis\n",
    "\n",
    "Text mining is the process of transforming unstructured text into structured data. This is essential, as the text must be in a machine-readable format. Now we can use various NLP tools to analyze that structured text.\n",
    "\n",
    "We cover two fundamental techniques of text analysis: topic modeling and word embeddings.\n",
    "\n",
    "## Python packages for text analysis\n",
    "\n",
    "Many Python libraries, packages, and modules help perform text analysis. Here is a brief breakdown of some of the most common ones:\n",
    "\n",
    "- **gensim:** Supports unsupervised NLP tasks, like topic modeling and document similarity analysis. Unlike spaCy or NLTK, which focus on syntactic NLP, Gensim is geared toward analysis via embedding-based NLP methods. Useful for tasks like topic modeling, document retrieval, text similarity, etc. [Gensim docs](https://radimrehurek.com/gensim/)\n",
    "\n",
    "- **pyLDAvis:** Supports interactive topic model visualization. This is useful for interpreting and evaluating model topics. Its visualization of topics is esepcially useful for understanding the distribution and relationships between topics. [pyLDAvis docs](https://pyldavis.readthedocs.io/en/latest/)\n",
    "\n",
    "- **word2vec:** transforms words into embedding vector points as a means of representing semantic relationships. It was developed by Google and is useful for analyzing potential semantic relationships between words. Word embeddings is fundamental to for understanding language models. We use gensim's word2vec module: [word2vec docs](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "\n",
    "### Install and import packages\n",
    "\n",
    "**Write code:** We will be using these packages for text mining and analysis in this session:\n",
    "\n",
    "- nltk\n",
    "- spacy\n",
    "- gensim\n",
    "- word2vec\n",
    "- pyLDAvis\n",
    "- string\n",
    "- re\n",
    "- string\n",
    "- os\n",
    "\n",
    "Write statements to install then import these packages and modules into our notebook environment. Keep in mind that string, re, and os come pre-loaded with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 60493,
     "status": "ok",
     "timestamp": 1743187420078,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "xIGoUh9-CfZT",
    "outputId": "23ad2f93-8f08-499d-bbbb-5a2ce7053f4c"
   },
   "outputs": [],
   "source": [
    "# install packages in colab env\n",
    "\n",
    "\n",
    "# import packages\n",
    "\n",
    "\n",
    "# load small English LM via spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lbHdW3zCfZU"
   },
   "source": [
    "## Load a corpus\n",
    "\n",
    "Before proceeding to text analysis, we need to load multiple text files from a directory as our corpus. To do this, we must first import the Python os module. This module allows users to interact with their operating systemâ€”e.g. navigate and manipulate file structures, run shell commands, etc.\n",
    "\n",
    "Here we import the os module and mount the Google Drive to our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1743187425633,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "0wuNuJ8OCfZU",
    "outputId": "a880c7ee-d6e6-4516-e13c-492c1b11508a"
   },
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1JVE9eYCfZU"
   },
   "source": [
    "Now we can load our directory of text files. We are here working with a complete collection of lyrics from all Taylor Swift's studio albums (up to 2023). We assign the subdirectory path to the variable `corpus_dir` and use the `listdir()` function from the os module to list all of the filenames in the tswift subdirectory.\n",
    "\n",
    "**Write code:**\n",
    "1. Define the `file_lst` variable as the list of all text files in our Taylor Swift subdirectory. To do this, call the `listdir()` function from the os module passing the variable `corpus_dir` as the function's argument. Remember that, in Python, we call specific items from modules/packages following the `large.small` syntactic structure.\n",
    "2. Write a print command to view the list by calling the `file_lst` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1743187428533,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "vgd1GLkUCfZU",
    "outputId": "77eb640b-e7dc-41b0-c9fa-dd31a3996e7a"
   },
   "outputs": [],
   "source": [
    "# define variable as subdir path\n",
    "corpus_dir = \"/gdrive/MyDrive/python-txt-humanities/text-data/tswift/\"\n",
    "\n",
    "# define variable as list of corpus text files\n",
    "\n",
    "\n",
    "# view the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsDEw95kCfZV"
   },
   "source": [
    "If we want to view the contents of one of these files at a time, we can use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1743187430075,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "VK8rGsvrCfZV",
    "outputId": "63f84490-01d4-4958-8367-bb256e366623"
   },
   "outputs": [],
   "source": [
    "with open('/gdrive/MyDrive/python-txt-humanities/text-data/tswift/Fifteen.txt', 'r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEFJMWGACfZV"
   },
   "source": [
    "Before we can do any text analysis, we need to do some simple, quick cleaning of our corpus. Look through some of the other text files. Print their contents and identify any patterns of noise you think you see that is common across the files.\n",
    "\n",
    "To print different files, you likely don't want to copy-paste or type out the whole file directory path. Fortunately, our directory path is saves as the `corpus_dir` variable. We can call this in our `open()` function using what is called an f-string. To write an f-string, type *f* before the string quotes, then inside the string call your variable with curly brackets `{}`. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1743187140941,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "zQnQtxrYCfZW",
    "outputId": "e491f0b2-c39b-4bef-8409-04258327ddb8"
   },
   "outputs": [],
   "source": [
    "with open(f'{corpus_dir}Fifteen.txt', 'r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfNKWAklCfZW"
   },
   "source": [
    "Replace `Fifteen.txt` with different text filenames from the corpus list.\n",
    "\n",
    "What patterns do you see that we can clean using regular expressions?\n",
    "\n",
    "## Clean multiple text files\n",
    "\n",
    "There are two features we can fix with regular expressions:\n",
    "\n",
    "1. First line of metadata found in each text file\n",
    "2. Song arrangement metadata marked with square brackets `[]`\n",
    "3. *Embed* statement preceded by a numerical string at end of file\n",
    "4. The string *See Taylor Swift LiveGet tickets as low as $60*\n",
    "\n",
    "**Write code:**\n",
    "1. Write a regular expression to replace multiple consecutive white space with one whitespace. You can use the re module's `sub()` function.\n",
    "2. Write a line of code that lowercases the whole text. Use Python's[string-type methods](https://docs.python.org/3/library/stdtypes.html).\n",
    "3. Add a print statement at the end that uses a list slice to print the first five texts from the `swift_discography` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1743188044935,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "CP407IYUCfZW",
    "outputId": "5035c8e7-3e37-44e3-8944-a6c749e7c653"
   },
   "outputs": [],
   "source": [
    "# create empty list to store lyrics\n",
    "swift_discography = []\n",
    "\n",
    "for song in file_lst:\n",
    "    file_path = corpus_dir + song\n",
    "    with open(file_path, \"r\", encoding='utf-8', errors='ignore') as file:\n",
    "        # read file, skipping first line\n",
    "        lines = file.readlines()[1:]\n",
    "        # join separate lines into one line\n",
    "        lyrics = ' '.join(lines)\n",
    "\n",
    "        # replace consecutive whitespace with a single white space\n",
    "        \n",
    "\n",
    "        # lowercase text\n",
    "        \n",
    "\n",
    "        # remove text bounded by square brackets (inclusive)\n",
    "        lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "\n",
    "        # remove \"embed\" when it is preceded by digits or special characters (inclusive)\n",
    "        lyrics = re.sub(r'[\\d?!]+embed', '', lyrics)\n",
    "\n",
    "        # remove concert advert\n",
    "        lyrics = re.sub(r'(?<=\\s|.)see taylor swift liveget tickets as low as \\$60(?=\\s|.)', '', lyrics)\n",
    "\n",
    "        # append lyrics to end of list\n",
    "        swift_discography.append(lyrics)\n",
    "\n",
    "# view first five items in list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJObrqHPG6SU"
   },
   "source": [
    "**Note on Python code:** See the regular expression for removing the concert advert. You may note the advert text `see taylor swift liveget tickets as low as \\$60` is preceded by to parenthetical sequences. These are called look-behind and look-ahead assertions:\n",
    "\n",
    "- `?<=\\s|.` This is a positive look-behind assertion, as signified by the characters `?<=`. The expression will look for any occurence of whitespace (`\\s`) or (`|`) any character (`.`) that precedes the advert text string.\n",
    "- `?=\\s|.` This is a positive look-ahead assertion, as signified by the characters `?=`. The expression will look for any occurence of whitespace (`\\s`) or (`|`) any character (`.`) that precedes the advert text string.\n",
    "\n",
    "Together, these will only find those instances of the advert that are preceded and followed by a whitespace or character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1743188047962,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "j63eLu7mCfZW",
    "outputId": "5bb53886-0956-4ab5-e2cf-638051133e2f"
   },
   "outputs": [],
   "source": [
    "# view first ten items on separate lines\n",
    "for lyrics in swift_discography[:10]:\n",
    "  print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS1k0HceCfZW"
   },
   "source": [
    "There is more we could do to clean the text. For our purposes, however, this is sufficient.\n",
    "\n",
    "We now need to put our text into a structured format for text analysis. Specifically, we can tokenize and lemmatize our text, as well as remove stopwords. Install these packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1743187648159,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "IeFR0HyACfZW",
    "outputId": "5d8671cb-0999-4f31-c4c8-86bbc7c682af"
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E1X5JaONHSM"
   },
   "source": [
    "**Write Python code:** Stopwords are words that are semantically less significant, either because they are filler or they are too common. But our corpus likely contains domain-specific stopwords not inlcuded in the NLTK stopword list. What are some words unique to our corpus that we may want to exclude from the texts? Add to the stoplist using the list object's `extend` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3054,
     "status": "ok",
     "timestamp": 1743188943898,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "OyCGok7iCfZX",
    "outputId": "efe12939-1b9f-4ed0-a22c-34a58f6d3d5c"
   },
   "outputs": [],
   "source": [
    "# POS tagger for lemmatization\n",
    "def pos_tagger(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def process_txt(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove non-alphabetic tokens\n",
    "    filtered_tokens_alpha = [word for word in tokens if word.isalpha()]\n",
    "    # Load NLTK stopword list and add original stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Extend Stopwords (Taylor's Version)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]\n",
    "    # Define lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Conduct POS tagging\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens_final)\n",
    "    # Lemmatize word-tokens via assigned POS tags\n",
    "    lemma_tokens = [lemmatizer.lemmatize(token, pos_tagger(pos_tag)) for token, pos_tag in pos_tags]\n",
    "    return lemma_tokens\n",
    "\n",
    "# Process each text in swift_discography\n",
    "processed_swift = [process_txt(text) for text in swift_discography]\n",
    "\n",
    "# Print the first 10 processed texts\n",
    "for doc in processed_swift[:10]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAfT7VmAFWX4"
   },
   "source": [
    "Our corpus is stored in a list of lists of lemmas. Each item in the list is a list representing one text, whose items in turn are lemmatized tokens from that text. It follows this structure:\n",
    "- corpus\n",
    "  - text_1\n",
    "    - token_1\n",
    "    - token_2\n",
    "    - token_n\n",
    "  - text_2\n",
    "    - token_1\n",
    "    - token_2\n",
    "    - token_n\n",
    "  - text_n\n",
    "  ...\n",
    "\n",
    "We can use this structured format to generate topic models and word vector models.\n",
    "\n",
    "But first, some background...\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "Most, if not all, NLP techniques utilize a feature extraction technique called Bag of Words (BoW). BoW models are an unstructured assortment of every word in a corpus/document defined solely by frequency. In other words, BoW models ignore grammar, syntax, word order, and co-occurence. i.e. ignoring word order and co-occurence.\n",
    "\n",
    "To construct a BoW from which we can make a topic model, we must first import certain packages and modules from the gensim library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHMVE3YRJQJF"
   },
   "outputs": [],
   "source": [
    "# import packages and modules for topic modeling\n",
    "import gensim.corpora as corpora\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "from pyLDAvis import gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIvDxb13J0LV"
   },
   "source": [
    "Now we can create a \"dictionary\" of the corpus. This is not a Python dictionary. Rather, it is an account of every word that appears across our corpus. We can then use this dictionary to construct the BoW.\n",
    "\n",
    "**Write Python code:** Write two items of code:\n",
    "1. If you run the code, the print statement returns a **TypeError**. A TypeError occurs in Python when one attempts to apply operations/functions to any inappropriate data type. Why do you think this line of code returns a TypeError? Rewrite the print statement using an f-string so that the string and list print together (separated by a line break). Line breaks are represented as `\\n`.\n",
    "2. Our code generates a BoW as a list of lists of word-frequency pairs. Each item of our main list is a list (representing a text) that is itself comprised of pairs of words and their frequencies. Write a two-line `for` statement that prints each list/document from our main list as a separate line so we can easily eyeball our BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1743194354843,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "HLjAUr-uJ0T6",
    "outputId": "d67399ef-fc4a-446b-c3af-ae60e2682c20"
   },
   "outputs": [],
   "source": [
    "# establish dictionary with gensim\n",
    "corpus_dict = corpora.Dictionary()\n",
    "\n",
    "# create BoW model from tokenized data\n",
    "corpus_bow = [corpus_dict.doc2bow(token, allow_update=True) for token in processed_swift]\n",
    "\n",
    "# view BoW, token IDs paired with the frequencies they appear in the document\n",
    "# modify this line\n",
    "print('(Token ID, Frequency):' + bow[0])\n",
    "\n",
    "# replace token ID with token string\n",
    "bow_freq = [[(corpus_dict[token_id], freq) for token_id, freq in pair] for pair in corpus_bow]\n",
    "\n",
    "# print each text's tokens+frequencies on individual lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Uim3vWXATF"
   },
   "source": [
    "Each line in the output is a separate document from our corpus; each line contains token-number pairs representing the tokens that appear in that document and the frequency with which they appear in that document.\n",
    "\n",
    "As we look over the lists, we may see semantically less-significant words reappearing throughout. We can add these to our custom stoplist, re-process the corpus, and re-create the BoW. After all, text processing and analysis is an interative process.\n",
    "\n",
    "**Write Python code:** There is another approach to remove semantically insignificant words. Gensim's dictionary module comes with a `filter_extremes()` function, described in the the [gensim docs](https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html). This function allows us to filter out words that appear under or above a certain threshold. We can call the function as a method of our dictionary variable (much like the `.lower()` method we use to lowercase text) before we create the BoW model. Read the documentation for `filter_extremes()` and call it as a method of the `corpus_dict` variable, filtering out tokens above and/or below a certain frequency threshold.\n",
    "\n",
    "## Topic models\n",
    "\n",
    "We can now create our topic model. Gensim's `LdaModel()` function provides a readily accessible method for topic model generation. The funciton allows us to customize the model parameters to better tune it to our data and purposes. To do this, we pass different arguments to the function. Gensim's [documentation](https://radimrehurek.com/gensim/models/ldamodel.html) provides information on all of the function arguments, but here is a brief overview of the select arguments we use:\n",
    "\n",
    "- `random_state`: known as 'seed' in many other packages and libraries. This ensures reproducibility. LDA is probabilistic, not deterministic. Even were we to train models with the same parameters on the same corpus, the resultant models would not be identical. But models trained with the same parameters on the same data will be the same *if* they are trained with the same random state.\n",
    "\n",
    "- `chunksize`: the number of texts the model function processes at one time\n",
    "\n",
    "- `num_topics`: number of topics into which the model distributes words and documents\n",
    "\n",
    "- `passes`: number of passes through the whole corpus\n",
    "\n",
    "Execute the models. This process might take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4047,
     "status": "ok",
     "timestamp": 1743196135001,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "pvhbFwHEUGOM",
    "outputId": "bcce8ce2-fb1a-4e6b-b0c9-5309a793bc65"
   },
   "outputs": [],
   "source": [
    "# import gensim models module\n",
    "from gensim import models\n",
    "\n",
    "# train LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus_bow,\n",
    "                     id2word=corpus_dict,\n",
    "                     random_state=43,\n",
    "                     chunksize=5,\n",
    "                     num_topics=7,\n",
    "                     passes=10,\n",
    "                     per_word_topics=True)\n",
    "\n",
    "# print topics\n",
    "for topic in lda_model.print_topics(num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0UFQ058jRpT"
   },
   "source": [
    "### Evaluate topic models\n",
    "\n",
    "Topic models can be evaluated qualitatively and quantitatively. For the former, an individual uses domain knowledge to \"eyeball\" top key terms for interpretability. Since topic models are only useful in so far as they can be interpreted, this sort of \"eyeballing\" is essential.\n",
    "\n",
    "This eyeballing can be done by simply glacning over the train output. There are other tools that can aid evaluation however. One is the pyLDAvis chart. This library visualizes every topic's scope and similarity in relation to one another by mapping them onto a two-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1743196142228,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "0SMD41pHjPDI",
    "outputId": "3af5dcd4-c619-42da-857c-b701a94cb86a"
   },
   "outputs": [],
   "source": [
    "# create an LDA Visualization Object\n",
    "lda_visual = pyLDAvis.gensim_models.prepare(lda_model, corpus_bow, corpus_dict)\n",
    "\n",
    "#display the visualization\n",
    "pyLDAvis.display(lda_visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd8o8GViEG85"
   },
   "source": [
    "\n",
    "Topic models can be evaluated qualitatively and quantitatively. Users can use domain knowledge to \"eyeball\" top key terms for interpretability. Common quantitiave metrics include log-likelihood and cohesion score, which measure topic probability and cohesion.\n",
    "\n",
    "Topic coherence is one common method for evaluating LDA topics. Topic coherence attempts to measure topic interpretability (i.e. \"coherence\"). Coherence scores care any value from 0 to 1, with 1 being perfect coherence and 0 being no coherence.\n",
    "\n",
    "Coherence sorts each topic's key terms from highest to lowest term weights. It then selects the first n terms in each topic and measures those terms similarity within their topic. How does the algorithm measure similarity? Cv is one widely adopted method that can be readily implemented via gensim's `CoherenceModel()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Okb2cg-EJhv"
   },
   "outputs": [],
   "source": [
    "# define coherence model\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=processed_swift, dictionary=corpus_dict, coherence='c_v')\n",
    "\n",
    "# obtain coherence scores\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# view coherence scores\n",
    "print(coherence_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rcxUGdoEOjI"
   },
   "source": [
    "### Tune topic model\n",
    "\n",
    "We can tune our topic model using model hyperparameters. Specifically, we can use the hyperparameters **alpha** and **eta**, which respectively influence document-topic and word-topic density.\n",
    "\n",
    "- **alpha:** this hyperparameter controls the document-topic density. A higher value smooths document weights over topics, meaning a document is more likely to contain a mixture of many topics. A lower value means that a document is more likely to contain only a few dominant topics.\n",
    "\n",
    "- **eta:** this hyperparameter controls word-topic density. A higher value smooths word distribution across topics, meaning that topics are likely to include a wider range of words from the corpus. A lower value means that topics are likely to be composed of only a few, heavily-weighted words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmm5PeGZ_adC"
   },
   "outputs": [],
   "source": [
    "# train LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus_bow,\n",
    "                     id2word=corpus_dict,\n",
    "                     random_state=43,\n",
    "                     chunksize=5,\n",
    "                     num_topics=7,\n",
    "                     passes=10,\n",
    "                     per_word_topics=True,\n",
    "                     alpha=10,\n",
    "                     eta=10)\n",
    "\n",
    "# print topics\n",
    "for topic in lda_model.print_topics(num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44hh_5fmlmUN"
   },
   "source": [
    "## Word embeddings\n",
    "\n",
    "Word embedding models represents words as numerical coordinate called *vectors*. Its aim is to represent words in such a way that their semantic relationships are preserved. The word-vectors are mapped across an multi-dimensional space in relation to one another. For this reason, the word vectors are also called *word embeddings*. Word embeddings works from the assumption that words with similar meanings appear in similar contexts. So each word is defined entirely by its relationship to every other word in the corpus. As a result, if one word establishes a new relationship, the \"definitions\" of all the other word embeddings also change.\n",
    "\n",
    "We use a package called word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnixhCUllyKg"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# instantiate an instance of the gensim Word2Vec class\n",
    "embeddings_model = gensim.models.Word2Vec(processed_swift, window=5, min_count=3)\n",
    "\n",
    "# set the model vocabulary from the gensim model and define a notification period\n",
    "embeddings_model.build_vocab(processed_swift, progress_per=10000)\n",
    "\n",
    "# now train the neural net\n",
    "embeddings_model.train(processed_swift, total_examples=embeddings_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPS4wiDcmMxX"
   },
   "source": [
    "Once we have our neural network trained, we can start asking questions about word semantics.   Play around with other words in which you might be interested.  But remember that this neural network was trained on a very small sample of texts, so don't put too much stock in what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxnb-gQNmHQQ"
   },
   "outputs": [],
   "source": [
    "# find the words most similar to a word of interest\n",
    "embeddings_model.wv.most_similar(positive=[\"man\"])\n",
    "\n",
    "# find the words most similar to a word of interest\n",
    "embeddings_model.wv.most_similar(negative=[\"man\"])\n",
    "\n",
    "# test the similarit of identified words\n",
    "embeddings_model.wv.similarity(\"man\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NUMtGCwYQ-T"
   },
   "source": [
    "Word embeddings are foundational to language models. For example, LLMs use word embeddings to generate text, retrieve texts, and fill gaps/masks. Albeit they use contextual word embeddings as opposed to word-type embeddings found in word2vec. Nevertheless, being able to understand and query word embeddings helps in understanding other NLP tools."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pythonpackageworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
