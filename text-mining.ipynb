{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RScxV--UlCxb"
   },
   "source": [
    "# Before we begin...\n",
    "\n",
    "This course introduces basic skills and methods of text mining and analysis, focusing most but not exclusively on analytic methods rooted in descriptive statistical analysis. As the name implies, Descriptive Statistical methods describe the basic features of the data being examined, providing simple but powerful summaries about the data.\n",
    "\n",
    "## About text analysis\n",
    "\n",
    "Some people interchangeably use *text mining*, *text analysis*, and *natural language processing* to refer to any large-scale textual analysis with computational tools. Within the humanities, terms such as *distant reading* (per Franco Moretti) or *macroanalysis* (per Matthew Jockers) are also substituted for any large-scale textual analysis. But these words all have subtle distinctions:\n",
    "\n",
    "- **Text Mining** refers to process of computationally \"reading\" a text (or collection of texts) and extracting specific chunks of information.  It encapsulates process that convert unstructured text to structured data.\n",
    "- **Text Analysis** refers to processes, such as basic descriptive statistical methods or more advanced machine learning methods, that analyze the information contained in texts.  Text Analysis is frequently performed on the structured data that results from Text Mining operations, but it can also be performed directly on unstructured texts.  Text Analysis processes typically return summary data, such as lists of all references to particular dates, persons, and topics or summary statistics regarding word or phrase usage and frequency.  \n",
    "- **Natural Language Processing** (NLP) describes a particular subset of Text Mining and Text Analysis processes that utilize the grammatical and semantic structures associated with natural languages as part of the mining and analysis process.  Processes that do not account for the naturalness of language, such as word frequency analysis, cannot properly be consider Natural Language Processing.\n",
    "\n",
    "## Data types and structures in Python\n",
    "\n",
    "Python uses four primary data types and structures:\n",
    "\n",
    "### Data types\n",
    "\n",
    "- String\n",
    "  - text\n",
    "- Integer\n",
    "  - whole numbers\n",
    "- Float\n",
    "  - decimals\n",
    "- Boolean\n",
    "  - binary values (e.g. True/False)\n",
    "\n",
    "### Data structures\n",
    "\n",
    "- Lists\n",
    "  - ordered, mutable, represented with `[]`\n",
    "- Dictionaries\n",
    "  - unordered, key-value pairs, represented with `{}`\n",
    "- Sets\n",
    "  - unordered, unique items, represented with `{}`\n",
    "- Tuples\n",
    "  - ordered, immutable, represented with `()`\n",
    "\n",
    "In this workshop, nearly everything we do is limited to strings and lists (save for some scant integers throughout).\n",
    "\n",
    "# Text mining\n",
    "\n",
    "## Python packages for text mining\n",
    "\n",
    "Many Python libraries, packages, and modules help perform text mining. Here is a brief breakdown of some of the most common ones:\n",
    "\n",
    "- **NLTK**: Supports common text processing techniques—e.g. tokenization, stemming/lemmatization, part-of-speech (PoS) tagging, etc. Also includes select datasets for training/evaluating NLP tools. Designed for more \"traditional\" NLP—e.g. sentiment analysis, named-entity recognition, etc. Supports multiple Latinate languages. [NLTK docs](https://www.nltk.org/)\n",
    "\n",
    "- **spaCy**: Supports much faster text processing. Includes pre-trained language models (LMs) for similar text processing tasks. Designed for production use with small-scale LMs—e.g. translation, summarization, etc. But does not support some functions like sentiment analysis. Supports multiple Latin and non-Latinate languages. [spaCy docs](https://spacy.io/)\n",
    "\n",
    "- **re**: Part of standard Python library. Supports regular expressions, which are useful as part of many text processing/cleaning tasks, e.g. string searching and matching, etc. Anything to do with patterns, this library helps. [re docs](https://docs.python.org/3/library/re.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3117,
     "status": "ok",
     "timestamp": 1626284521948,
     "user": {
      "displayName": "Jacob Murel",
      "photoUrl": "",
      "userId": "08304125786498900321"
     },
     "user_tz": 240
    },
    "id": "ZGDtzniRIEix",
    "outputId": "99e3428f-92eb-4864-81b4-eebfab33d595"
   },
   "outputs": [],
   "source": [
    "# install packages in colab env\n",
    "%pip install nltk -U\n",
    "%pip install spacy -U\n",
    "\n",
    "# import packages\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeXFdNBIu011"
   },
   "source": [
    "## Load a text file\n",
    "\n",
    "Both NLTK and spaCy allow users to perform some initial processing on text files as they're loaded. For now, however, in order to see the effects of text processing, we'll simply load a text file sans any load-time processing.\n",
    "\n",
    "Since we are working in Google Drive, we need to first mount the drive to our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3325,
     "status": "error",
     "timestamp": 1743691306431,
     "user": {
      "displayName": "Jacob Murel",
      "userId": "13050471245292509150"
     },
     "user_tz": 240
    },
    "id": "5m5fdnh2vAQW",
    "outputId": "4af19a26-a0f8-4c6c-e355-187687b03297"
   },
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MGEZCbfvMsS"
   },
   "source": [
    "**Note on Python syntax:** In the code `from google.colab import drive`, we import the drive module from the colab package of the google Python library.  If we want to call specific items from imported modules, packages, or libraries, we must use the Python structure `large.small`, with `.` between each unit. So take this code: `from google.colab import drive`. Here, we tell Python to go to google library, find the colab pacakge, and import only its drive module/function.\n",
    "\n",
    "Now that you've mounted your drive, you can open files from the course folder in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enKrUpF4wZwm"
   },
   "outputs": [],
   "source": [
    "src_txt_path = \"/gdrive/MyDrive/python-nlp-humanities/text-data/melville.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWwLexukweVo"
   },
   "source": [
    "We've now stored the text file as a Python variable.\n",
    "\n",
    "*What is a variable?*\n",
    "\n",
    "A Python variable is a symbolic name used to reference a data value. Once we assign a variable, we can use that variable to to access and manipulate that value elsewhere.\n",
    "\n",
    "There are some general rules when naming variables in Python:\n",
    "\n",
    "- variable names must begin with a letter or underscore\n",
    "- variable names are case-sensitive\n",
    "\n",
    "From here, we can now open the file, read its contents, and save those contents as a new Python string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNRlMPS1Wzi3"
   },
   "outputs": [],
   "source": [
    "# read file\n",
    "src_txt = open(src_txt_path, \"r\")\n",
    "\n",
    "# \"extract\" file contents as string variable\n",
    "working_txt = src_txt.read()\n",
    "\n",
    "# confirm file loaded\n",
    "print('Characters in text:', len(working_txt))\n",
    "\n",
    "# view first 500 characters of \"extracted\" text\n",
    "print(working_txt[:500:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7uZRUI9zLHo"
   },
   "source": [
    "**Note on Python indices:** Python can iterate through data structures, such as strings and lists, using indices. *Index* refers to a given item's position within a sequence. Python indices begin at 0 (e.g. the first character in a string is at index 0). The code `[:500:1]` is called a split. Splits allow us to select a specific subsection of a sequence. Splits follow this structure: `[start:stop:step]`, defined as:\n",
    "- `start`: start index (inclusive; default = 0)\n",
    "- `stop`: end index (exclusive; default = -1)\n",
    "- `step`: increment of indices (default = 1)\n",
    "\n",
    "...Back to text processing.\n",
    "\n",
    "The foundational but overlooked first step in any text analysis pipeline is forensics. You must be familiar with your data—its format, what noise may exist therein, the structure and patterns to which it may adhere, etc. This will aid you, not only in interpreting any later results you obtain, but also in cleaning the text to ensure you obtain valid results.\n",
    "\n",
    "Look at the 500 characters and think of features in the text that could pose problems for later analysis.\n",
    "\n",
    "There are several computational tools/methods for performing text analysis. These include analyzing word co-occurence, word/n-gram distributions. Since I am already familiar with the primary issues afflicting out .txt file, and for the sake of time, we'll jump to common text cleaning tasks. As we move forward, consider how you may apply these text cleaning objectives to your own text(s).\n",
    "\n",
    "## Text cleaning\n",
    "\n",
    "### Paratext\n",
    "\n",
    "Our first task is to remove the Project Gutenbeg header and footer, what narratologist Gérard Genette calls *paratext*. Rather than opt for cumbersome rule-based approach (e.g. character counting, regular expressions, etc.), we can load the [gutenbergpy](https://github.com/raduangelescu/gutenbergpy) library, which contains features designed to strip PG files of their paratext.\n",
    "\n",
    "**Write code:** To load the gutenbergpy library:\n",
    "1. Use pip to install the gutenburgpy library\n",
    "2. Import the `textget()` function from the gutenbergpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0g_6MQvuFxJ2",
    "outputId": "1fb9e337-dd9c-4f1e-800a-ba304435d013"
   },
   "outputs": [],
   "source": [
    "# install gutenbergpy\n",
    "\n",
    "\n",
    "# import textget()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7_PBYSTSfWh"
   },
   "source": [
    "**Write code:** Write a print statement that uses a slice to view the first 500 characters (i.e. indices) from the `txt_body` string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wth0eGCJFxJ2"
   },
   "outputs": [],
   "source": [
    "# strip PG header\n",
    "txt_body = textget.strip_headers(working_txt.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "# strip PG footer\n",
    "txt_body = re.sub(r'end of the project gutenberg', '', txt_body, flags=re.IGNORECASE)\n",
    "\n",
    "# view first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK0OpAzcFxJ3"
   },
   "source": [
    "**Note on functions:** The re module `sub()` function replaces one text string with another. Nearly all functions take arguments. We discuss functions in more detail later, but for now know that arguments are the items you give to a function inside of its parentheses. The `sub()` function takes, at minimum, three arguments: `(string_in_text, replacement_string, input_text)`.\n",
    "\n",
    "\n",
    "### Whitespace and cases\n",
    "\n",
    "Cases and whitespace can also cause problems in some NLP methods. For example, some NLP pipelines separate text strings based on line divisions, but we may not care about paragraph divisions.\n",
    "\n",
    "**Write a code:** Write two regular expressions for both `clean_txt` variables:\n",
    "1. The first should replace all line breaks with a single whitespace character.\n",
    "    - The regular expression character for line breaks is `\\n`.\n",
    "2. The second should replace occurences of two or more whitespace characters with a single whitespace character.\n",
    "    - Consult the [regular expression documentation](https://docs.python.org/3/library/re.html) to find how to represent whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UQrXoHlFxJ3"
   },
   "outputs": [],
   "source": [
    "# lowercase text\n",
    "lower_txt = txt_body.lower()\n",
    "\n",
    "# turn new lines into single space\n",
    "\n",
    "# turn subsequent whitespace into single space\n",
    "\n",
    "# remove any leading/trailing whitespace\n",
    "clean_txt = clean_txt.strip()\n",
    "\n",
    "# view first 500 characters\n",
    "print(txt_body[:500:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGy7f8VqFxJ3"
   },
   "source": [
    "**Note on Python method:** Certain objects and data types in Python have their own functions call *methods*. These are function's that can only be called for a given object or data type. Here, we use the `lower()` method and the `strip()` method, which are methods for string objects. Here is the [Python documentation](https://docs.python.org/3/library/stdtypes.html#string-methods) on string object methods. Keep in mind that each data type and structure has its own methods.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Before we move onto more robust cleaning, we'll first tokenize the text. Tokenization turns unstructured text data in structured text data comprised of units called *tokens*. A token is simply a text-based unit. It can have almost any pre-defined size, ranging from a single character, words, paragraphs, and so forth.\n",
    "\n",
    "**Write code:** Write a print statement that uses a slice to view the first 100 indices from the `tokens` variable list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGSerFZFxJ3"
   },
   "outputs": [],
   "source": [
    "# import NLTK word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenize on words\n",
    "tokens = word_tokenize(clean_txt)\n",
    "\n",
    "# view first 100 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlcTJbUWFxJ4"
   },
   "source": [
    "Look through the list of tokens. What do you notice? Are there any tokens or bits of text that may create noise? We can remove these tokens through a combination of additional preprocessing steps.\n",
    "\n",
    "For now, we'll start with numbers. Execute this code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQiRUHKgFxJ4"
   },
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "filtered_tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# remove non-alphabetic tokens\n",
    "filtered_tokens_alpha = [word for word in filtered_tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKwzDVt4FxJ4"
   },
   "source": [
    "**Note on Python syntax:** This code may seem a lot. But let's break it down to better understand Python syntactic structure. Let's look specifically at this code:\n",
    "```\n",
    "filtered_tokens = [word for word in tokens if word not in string.punctuation]\n",
    "```\n",
    "This sets up a list variable `filtered_tokens`. `word` is simply another variable that refers to the items in this and other lists. So we create a list variable named \"filtered_tokens\" whose items are the same items in the \"tokens\" list if those items are not in the string module's \"punctuation\" list.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Stopwords are common, semantically less significant words removed from a text set. This includes articles (*the* and *a*), *to be* conjugations (*is* and *are*), conjunctions (*and*), and some repositions (*of*). These are compiled into a stop list, i.e. list of stopwords. A stop list typically consists of the most common words in a given language, which are believed to add little value to a text's overall meaning.\n",
    "\n",
    "Rather than create a wholly original stop list, we can load the NLTK English language \"stopwords\" list.\n",
    "\n",
    "**Write code:** Write a single line of code to download the NLTK \"stopwords\" list using the `download()` function from the NLTK library. Per the NLTK [downloader documentation](https://www.nltk.org/api/nltk.downloader.html), you will need to pass \"stopwords\" as an argument to the `download()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku-TXdwHFxJ4"
   },
   "outputs": [],
   "source": [
    "# import NLTK stopword module\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download NLTK stopwords list\n",
    "\n",
    "\n",
    "# load NLTK stopwords to variable\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# view stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20qGGdLRFxJ4"
   },
   "source": [
    "\n",
    "\n",
    "**Write code:** Review the [Python documentation](https://docs.python.org/3/tutorial/datastructures.html) for list object methods. What method allows you to add multiple items to a pre-existing list object? Use the appropriate method to add your own list of additional stopwords to NLTK's pre-defined stoplist. Keep in mind that, because `stop_words` is a list object, you need to compile and add these additional stopwords as a Python list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNSV-SM1FxJ4"
   },
   "outputs": [],
   "source": [
    "# add corpus-specific stopwords to list\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "filtered_tokens_final = [word for word in filtered_tokens_alpha if not word in stop_words]\n",
    "\n",
    "# view first 50 filtered tokens\n",
    "print(filtered_tokens_final[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnFjpjKSFxJ5"
   },
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "How does one treat variants of a single word/concept (dancer, dances, dancing, etc.) as equivalent? Stemming and lemmatization are different approaches to this problem. They both reduce inflected word forms to a common root word (or, *lemma*). But here are key differences:\n",
    "\n",
    "Stemming is a fairly heuristic approach to essentially removes suffixes (e.g. *-ing*, *-s*, etc.) to produce a base form for each word.\n",
    "\n",
    "Lemmatization utilizes part-of-speech (POS) tagging to reduce inflected forms. POS assigns each word/token a tag that represents its grammatical function. It then uses this tag to identify a word's base \"dictionary form.\"\n",
    "\n",
    "There are multiple stemming algorithms, with various strengths and weaknesses. One notable example is the Porter stemmer, which is fairly straightforward to import and execute.\n",
    "\n",
    "Stemming can produce notable erroneous examples however. So let's move ahead with lemmatization.\n",
    "\n",
    "**Write code:** Write import and download statements to import the necessary modules and functions from NLTK:\n",
    "1. import \"WordNetLemmatizer\" from the NLTK \"stem\" package\n",
    "2. import \"wordnet\" from the NLTK \"corpus\" package\n",
    "3. download the \"averaged_perceptron_tagger\" from NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SMqzUmjPkxx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ZePp84Pk9A"
   },
   "source": [
    "Now we can pass our text through the lemmatizer.\n",
    "\n",
    "Fortunately, we do not need to create a lemmatization and POS algorithm. We can assign POS tags to words using the Penn treebank and pass them through a pre-created WordNet lemmatizer. This reduces words to base forms according to their assigned part of speech. In this way, lemmatization accounts for some of the errors that arise from stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8TIrIxaFxJ5"
   },
   "outputs": [],
   "source": [
    "# POS tagger function\n",
    "def pos_tagger(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(filtered_tokens_final)\n",
    "\n",
    "# lemmatize tokens with POS tags\n",
    "lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n",
    "\n",
    "# view first 500 lemmas\n",
    "print(lemma_tokens[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWt1bjE0FxJ5"
   },
   "source": [
    "## Functions\n",
    "\n",
    "In Python, functions are re-callable blocks of code designed to execute a specific task. Functions are useful for organizing larger workflows into discrete, step-wise tasks.\n",
    "\n",
    "We create a function using the `def` keyword, followed by the function's name and a pair of parentheses `()`. Inside the parentheses, we can include any variables we want to pass to the function, such as for processing data—these variables are called *arguments*. All code inside the function is indented. We close the function with a `return` statement—this specifies the function's output.\n",
    "\n",
    "**Write code:** Create a function for text cleaning. The function should execute the six-item action list described in the function's doc string. You may use the code we have reviewed to create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foyj3CVzFxJ5"
   },
   "outputs": [],
   "source": [
    "def pg_txt_cleaner(text: str):\n",
    "    \"\"\"\n",
    "    Cleans contents of an input .txt file sourced from Project Gutenberg by:\n",
    "        1. remove PG header + footer\n",
    "        2. lowercase text\n",
    "        3. regularize whitespace\n",
    "        4. tokenize text\n",
    "        5. remove punctuation and stopwords\n",
    "        6. lemmatize tokens using WordNet\n",
    "\n",
    "    Arguments:\n",
    "        text (str) -- input .txt file\n",
    "    Returns:\n",
    "        lemma_tokens (list) -- list of filtered and lemmatized tokens\n",
    "    \"\"\"\n",
    "\n",
    "    working_txt = text.read()\n",
    "\n",
    "    ## write text preprocessing code here\n",
    "\n",
    "    return lemma_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKCJ9U6iFxJ5"
   },
   "source": [
    "**Note on function arguments:** Our function takes a single variable called \"text\". Notice the \"str\" keyword following the variable. This is called a *type annotation*. Type annotations allow you to restrict a function's input to a specific data type in order to prevent potential downstream errors. To create a type annotation, simply write the variable, followed by a colon ( **:** ) and the data type keyword (i.e. str, int, float, bool).\n",
    "\n",
    "## Conditionals in Python\n",
    "\n",
    "Per its name, conditionals execute blocks of code only if certain conditions are met. This serves as a way to implement what is called a *control structure*, and allows us to automate decisions that direct a program's order of execution. There are three components to Python conditionals:\n",
    "\n",
    "\n",
    "- `if`: checks a condition. If the condition is met (i.e. True), the subsequent portion of code executes.\n",
    "- `elif`: checks an additional following the initial if-condition. elif statements are used only if you intend to check more than one condition.\n",
    "- `else`: specifies what code to execute if none of the preceding conditions (if or elif) are met.\n",
    "\n",
    "\n",
    "**Write docstrings:** Write documentation for the following function that uses conditionals. Write a doc string for the function as a whole. It should include a description (1-2 sentences) of the function's action/purpose, lists its arguments, and note what the function returns. Wherever you encounter an empty line, write an in-line comment describing the following conditional statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R3NPbVtFxJ5"
   },
   "outputs": [],
   "source": [
    "def pos_tagger(treebank_tag):\n",
    "    \"\"\"\n",
    "    [Description]\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34fv6OAAFxJ5"
   },
   "source": [
    "## Loops in Python\n",
    "\n",
    "Loops similarly use conditions, albeit in a different way. Loops direct the program to repeat some action/code so long as a certain condition is held. There are *while* loops and *for* loops:\n",
    "\n",
    "- `for`: iterates over a data structure and executes the following code for each item in that structure\n",
    "- `while`: execute the following code so long as a specified condition is met\n",
    "\n",
    "\n",
    "**Write docstrings:** This function uses a for loop and a conditional. Write documentation for the following function that uses a loop. Write a doc string for the function as a whole. It should include a description (1-2 sentences) of the function's action/purpose, lists its arguments, and note what the function returns. Wherever you encounter an empty line, write an in-line comment describing what the following line/block of code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwBgV0KYFxJ6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_txt_files(dir: str):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for file_name in os.listdir(dir):\n",
    "\n",
    "        if file_name.endswith('.txt'):\n",
    "            with open(os.path.join(dir, file_name), 'r', encoding='utf-8') as file:\n",
    "\n",
    "                clean_tokens = pg_txt_cleaner(file)\n",
    "\n",
    "                texts.append(clean_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCZuPnhsFxJ6"
   },
   "source": [
    "**Write code**: Our function uses an if conditional without an else statement. This is not necessarily a problem—the code executes fine. But it is good practice to include an else statement whenever you use if conditionals in order to handle exceptions. Write an else statement for the end of this function. It should tell the user a file has not been processed because it is not a .txt file.\n",
    "\n",
    "We can now move on to some classic NLP text analysis."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pythonpackageworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
